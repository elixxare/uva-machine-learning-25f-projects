# -*- coding: utf-8 -*-
"""LyricSentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EmzuYqxL5_rJq9hVN9DDXvxF59oK20AN

# Lyric Sentiment
Description:
"""

import os
import json
import pandas as pd
import torch
from transformers import pipeline
from tqdm import tqdm

KAGGLE_USERNAME = "ibaadhassan"
KAGGLE_KEY      = "KGAT_5f546bb00a4661f75d64b33b4964ad8d"

if KAGGLE_USERNAME == "paste_your_username_here":
    raise ValueError("âš ï¸ STOP: You forgot to paste your Kaggle Username and Key in Step 0!")

if not os.path.exists('/root/.kaggle/kaggle.json'):
    print("ðŸ”„ Setting up Kaggle credentials automatically...")

    !mkdir -p ~/.kaggle
    creds = {"username": KAGGLE_USERNAME, "key": KAGGLE_KEY}

    with open("kaggle.json", "w") as f:
        json.dump(creds, f)

    !mv kaggle.json ~/.kaggle/
    !chmod 600 ~/.kaggle/kaggle.json
    print("âœ… Login successful. Credentials saved.\n")
else:
    print("âœ… Already logged in.\n")

TARGET_ARTISTS = [
    'Taylor Swift', 'Kendrick Lamar', 'Drake',
    'Bad Bunny', 'The Weeknd', 'Billie Eilish'
]

if not os.path.exists('song_lyrics.csv'):
    print("Downloading Genius Dataset (~9GB)... This may take 2-5 minutes.")
    !kaggle datasets download -d carlosgdcj/genius-song-lyrics-with-language-information
    !unzip -o genius-song-lyrics-with-language-information.zip

    import glob
    csv_files = glob.glob("*.csv")
    if csv_files:
        largest_csv = max(csv_files, key=os.path.getsize)
        os.rename(largest_csv, 'song_lyrics.csv')
    print("Download complete.\n")

print(f"Filtering dataset for artists: {TARGET_ARTISTS}...")
filtered_chunks = []

try:
    for chunk in tqdm(pd.read_csv('song_lyrics.csv', chunksize=100_000)):
        chunk.columns = [c.lower() for c in chunk.columns]
        if 'artist' in chunk.columns:
            subset = chunk[chunk['artist'].isin(TARGET_ARTISTS)].copy()
            filtered_chunks.append(subset)

    if filtered_chunks:
        df_lyrics = pd.concat(filtered_chunks)
        print(f"âœ… Data loaded! Found {len(df_lyrics)} songs.")
        display(df_lyrics.head(3))
    else:
        print("âš ï¸ No songs found. Please check artist names/spelling.")
        df_lyrics = pd.DataFrame(columns=['artist', 'lyrics'])

except FileNotFoundError:
    print("âŒ Error: 'song_lyrics.csv' not found.")

if not df_lyrics.empty:
    print("\nInitializing DistilBERT Sentiment Model...")

    sentiment_analyzer = pipeline(
        "sentiment-analysis",
        model="distilbert-base-uncased-finetuned-sst-2-english",
        device=0 if torch.cuda.is_available() else -1
    )

    def get_sentiment_label(text):
        if not isinstance(text, str): return "NEUTRAL"
        result = sentiment_analyzer(text, truncation=True, max_length=512)[0]
        return result['label']

    print("Running analysis on a sample (first 50 songs per artist)...")
    df_sample = df_lyrics.groupby('artist').head(50).copy()

    tqdm.pandas()
    df_sample['sentiment'] = df_sample['lyrics'].progress_apply(get_sentiment_label)

    print("\nCalculating Sentiment Distribution...")
    sentiment_counts = df_sample.groupby(['artist', 'sentiment']).size().unstack(fill_value=0)
    sentiment_percents = sentiment_counts.div(sentiment_counts.sum(axis=1), axis=0) * 100

    display(sentiment_percents)

    sentiment_percents.plot(
        kind='bar',
        stacked=True,
        color=['red', 'green'],
        title='Lyric Sentiment Distribution by Artist',
        ylabel='Percentage (%)',
        xlabel='Artist'
    )
    print("\nâœ… Part 1.1 Complete!")

# # ==========================================
# # PART 1.1 CONTINUED: STEPS 3 & 4
# # ==========================================

# # We assume 'df_lyrics' is already loaded from your previous cell.
# # If you see an error saying "df_lyrics not defined", then you DO need to re-run the first cell.

# if 'df_lyrics' in locals() and not df_lyrics.empty:
# #     print("âœ… Data found in memory. Proceeding to Sentiment Analysis...")
# #     print("Initializing DistilBERT Sentiment Model...")

# #     # Load the pre-trained sentiment model
# #     # This downloads a ~250MB model from Hugging Face
#     sentiment_analyzer = pipeline(
#         "sentiment-analysis",
#         model="distilbert-base-uncased-finetuned-sst-2-english",
#         device=0 if torch.cuda.is_available() else -1
#     )

#     def get_sentiment_label(text):
#         if not isinstance(text, str): return "NEUTRAL"
#         # Truncate to 512 tokens to fit BERT limits
#         result = sentiment_analyzer(text, truncation=True, max_length=512)[0]
#         return result['label']

# #     print("Running analysis on a sample (first 50 songs per artist)...")
# #     # We take the first 50 songs per artist to make this fast.
# #     # To analyze ALL 2000+ songs, remove the line below.
#     df_sample = df_lyrics.groupby('artist').head(50).copy()

# #     # Enable progress bar
#     tqdm.pandas()
#     df_sample['sentiment'] = df_sample['lyrics'].progress_apply(get_sentiment_label)

# #     # ------------------------------------------
# #     # STEP 4: VISUALIZE RESULTS
# #     # ------------------------------------------
# #     print("\nCalculating Sentiment Distribution...")

# #     # Create a table of counts
#     sentiment_counts = df_sample.groupby(['artist', 'sentiment']).size().unstack(fill_value=0)

# #     # Convert to percentages
#     sentiment_percents = sentiment_counts.div(sentiment_counts.sum(axis=1), axis=0) * 100

# #     print("\nSentiment Percentages per Artist:")
# #     display(sentiment_percents)

# #     # Plot the chart
#     sentiment_percents.plot(
#         kind='bar',
#         stacked=True,
#         color=['red', 'green'],
#         title='Lyric Sentiment Distribution by Artist',
#         ylabel='Percentage (%)',
#         xlabel='Artist'
#     )
# #     print("\nâœ… Part 1.1 Complete!")

# else:
# #     print("âš ï¸ Error: 'df_lyrics' is missing. Please re-run the Download & Filter cell first.")

# print("Installing Kaggle API client library...")
# !pip install kaggle
# print("Kaggle API client library installed.")

"""### Upload Kaggle API Key

To interact with Kaggle, you need to upload your `kaggle.json` file. This file contains your API credentials.

**Instructions:**
1.  Go to Kaggle.com, log in, and navigate to your account page (`Account` -> `Create New API Token`).
2.  This will download a `kaggle.json` file to your computer.
3.  In Colab, click on the folder icon on the left sidebar (`Files`).
4.  Click the `Upload to session storage` icon (the file with an arrow pointing up).
5.  Select the `kaggle.json` file you just downloaded and upload it.

Once uploaded, the `kaggle.json` file should be visible in your Colab file browser.

**Reasoning**:
After the `kaggle.json` file is uploaded, the environment variables need to be set up so that the Kaggle API can correctly find and use the authentication credentials. This involves creating the `.kaggle` directory, moving the `kaggle.json` file into it, and setting the appropriate file permissions.
"""

# print("Setting up Kaggle API environment variables...")
# !mkdir -p ~/.kaggle
# !mv kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json
# print("Kaggle API environment variables set up.")

# print("Downloading 'reddit-music-discourse' dataset...")
# !kaggle datasets download -d prajwalmr/reddit-music-discourse
# print("'reddit-music-discourse' dataset downloaded.")

"""## Download Reddit Music Discourse Dataset

### Subtask:
Download the 'reddit-music-discourse' dataset from Kaggle, unzip it, and load it into a pandas DataFrame.

**Reasoning**:
The previous attempt to download the dataset failed due to a 403 error, possibly indicating an incorrect dataset path. The instructions now provide an updated dataset path, so the next logical step is to re-attempt the download with the corrected path.
"""

# print("Downloading 'reddit-music-discourse' dataset from bhavyajeet...")
# !kaggle datasets download -d bhavyajeet/reddit-music-discourse
# print("'reddit-music-discourse' dataset downloaded.")

"""**Reasoning**:
The 'reddit-music-discourse' dataset has been downloaded as a zip file. The next step is to unzip it and then load the CSV file within into a pandas DataFrame, followed by displaying its head to confirm successful loading.


"""

# print("Unzipping 'reddit-music-discourse.zip'...")
# !unzip -o reddit-music-discourse.zip
# print("'reddit-music-discourse.zip' unzipped.")

# import pandas as pd

# print("Loading 'reddit-music-discourse.csv' into a DataFrame...")
# df_reddit = pd.read_csv('reddit-music-discourse.csv')
# print("DataFrame 'df_reddit' loaded. Displaying head:")
# display(df_reddit.head())

"""**Reasoning**:
The previous attempt to load the CSV failed because the specified file 'reddit-music-discourse.csv' was not found directly in the current directory after unzipping. The unzip output indicates that the relevant CSV file is likely 'data/songs.csv' or 'data/clustered_posts/miscellaneous/index.csv'. Given the dataset name 'reddit-music-discourse', 'data/songs.csv' seems to be the most appropriate file to load. The code will be modified to load the correct path and display the head of the DataFrame.


"""

# print("Unzipping 'reddit-music-discourse.zip'...")
# !unzip -o reddit-music-discourse.zip
# print("'reddit-music-discourse.zip' unzipped.")

# import pandas as pd

# print("Loading 'data/songs.csv' into a DataFrame...")
# df_reddit = pd.read_csv('data/songs.csv')
# print("DataFrame 'df_reddit' loaded. Displaying head:")
# display(df_reddit.head())

"""**Reasoning**:
The 'reddit-music-discourse' dataset has been successfully loaded. The next logical step is to download the second dataset required by the main task, 'tweets-from-music-artists-balanced-dataset'.


"""

# print("Downloading 'tweets-from-music-artists-balanced-dataset'...")
# !kaggle datasets download -d abdelrahmanali12/tweets-from-music-artists-balanced-dataset
# print("'tweets-from-music-artists-balanced-dataset' downloaded.")

"""### Kaggle Dataset Download Failed

The download for the 'tweets-from-music-artists-balanced-dataset' dataset failed with a `403 Client Error: Forbidden`. This error usually occurs if:

*   The dataset ID (`abdelrahmanali12/tweets-from-music-artists-balanced-dataset`) is incorrect.
*   The dataset is private and requires specific permissions or credentials not currently available.
*   The dataset has been removed or is no longer publicly accessible.

Please verify the dataset ID and ensure it is publicly available, or provide an alternative dataset ID if the original one is no longer valid or accessible. Once confirmed, you can re-attempt the download.

## Download Tweets from Music Artists Dataset

### Subtask:
Download the 'tweets-from-music-artists-balanced-dataset' from Kaggle, unzip it, and load it into a pandas DataFrame.

**Reasoning**:
The previous attempt to download the 'tweets-from-music-artists-balanced-dataset' failed due to a 403 error, likely caused by an incorrect dataset path. The instructions now provide an updated path, so the next logical step is to re-attempt the download with the corrected Kaggle dataset identifier.
"""

# print("Downloading 'tweets-from-music-artists-balanced-dataset'...")
# !kaggle datasets download -d alejandroservin/tweets-from-music-artists-balanced-dataset
# print("'tweets-from-music-artists-balanced-dataset' downloaded.")

"""**Reasoning**:
The 'tweets-from-music-artists-balanced-dataset' has been successfully downloaded as a zip file. The next step is to unzip it, identify the correct CSV file within the unzipped contents, load it into a pandas DataFrame named `df_tweets`, and display its head to confirm successful loading.


"""

# print("Unzipping 'tweets-from-music-artists-balanced-dataset.zip'...")
# !unzip -o tweets-from-music-artists-balanced-dataset.zip
# print("'tweets-from-music-artists-balanced-dataset.zip' unzipped.")

# import pandas as pd

# # Assuming the CSV file is named 'tweets_from_music_artists_balanced_dataset.csv' or similar within the unzipped folder.
# # If this path is incorrect, further inspection of the unzipped directory will be needed.
# # Based on typical Kaggle dataset structures, it might be in a 'data/' directory or directly in the unzipped folder.
# # Let's try common names first. If it fails, we will list directory contents.

# # First attempt: check for the most common naming convention.
# csv_file_name = 'tweets_from_music-artists_balanced-dataset.csv'

# try:
# #     print(f"Loading '{csv_file_name}' into a DataFrame...")
#     df_tweets = pd.read_csv(csv_file_name)
# #     print("DataFrame 'df_tweets' loaded. Displaying head:")
# #     display(df_tweets.head())
# except FileNotFoundError:
#     # If the direct load fails, try a common alternative or list directory contents to find the correct path.
# #     print(f"'{csv_file_name}' not found. Listing unzipped contents to find the correct CSV file...")
#     !ls -R
#     # Based on observation, the CSV is often just 'tweets.csv' or 'dataset.csv' inside the unzipped folder
#     # or a 'data' folder. From previous execution, the unzipped file is usually in /content.
#     csv_file_name = 'tweets.csv'
#     try:
# #         print(f"Attempting to load '{csv_file_name}' into a DataFrame...")
#         df_tweets = pd.read_csv(csv_file_name)
# #         print("DataFrame 'df_tweets' loaded. Displaying head:")
# #         display(df_tweets.head())
#     except FileNotFoundError:
#         csv_file_name = 'dataset.csv'
#         try:
# #             print(f"Attempting to load '{csv_file_name}' into a DataFrame...")
#             df_tweets = pd.read_csv(csv_file_name)
# #             print("DataFrame 'df_tweets' loaded. Displaying head:")
# #             display(df_tweets.head())
#         except FileNotFoundError:
# #             print("Could not find a common CSV file. Please manually inspect the directory and specify the correct path.")

"""**Reasoning**:
The previous attempt to load the tweets dataset failed because the correct CSV filename was not identified. Based on the output of `!ls -R`, the correct filename is 'tweets-from-music-artists_balanced-dataset.csv'. The next step is to load this file into a pandas DataFrame.


"""

# import pandas as pd

# print("Loading 'tweets-from-music-artists_balanced-dataset.csv' into a DataFrame...")
# df_tweets = pd.read_csv('tweets-from-music-artists_balanced-dataset.csv')
# print("DataFrame 'df_tweets' loaded. Displaying head:")
# display(df_tweets.head())

"""## Final Task

### Subtask:
Confirm that both Kaggle datasets have been successfully downloaded, unzipped, and loaded into pandas DataFrames, ready for the next steps of filtering and cleaning.

## Summary:

### Q&A
No, both Kaggle datasets were not successfully downloaded and loaded on the first attempt due to `403 Client Error: Forbidden` issues with incorrect or restricted dataset IDs. However, after identifying alternative accessible datasets and paths, both datasets were ultimately downloaded, unzipped, and loaded into pandas DataFrames.

### Data Analysis Key Findings
*   The Kaggle API environment was successfully set up, creating the `~/.kaggle` directory and configuring `kaggle.json` with appropriate permissions.
*   The initial attempt to download the 'reddit-music-discourse' dataset (ID: `prajwalmr/reddit-music-discourse`) failed with a `403 Client Error: Forbidden`.
*   A subsequent attempt using an alternative ID (`bhavyajeet/reddit-music-discourse`) successfully downloaded, unzipped, and loaded the 'reddit-music-discourse' dataset into a DataFrame named `df_reddit` from the `data/songs.csv` file.
*   An attempt to download the 'tweets-from-music-artists-balanced-dataset' dataset using the ID `abdelrahmanali12/tweets-from-music-artists-balanced-dataset` also resulted in a `403 Client Error: Forbidden`.
*   Finally, the 'tweets-from-music-artists-balanced-dataset' was successfully downloaded, unzipped, and loaded into a DataFrame named `df_tweets` using the ID `alejandroservin/tweets-from-music-artists-balanced-dataset` and identifying the correct CSV file as `tweets-from-music-artists_balanced-dataset.csv`.

### Insights or Next Steps
*   The primary task of downloading both specified datasets and loading them into DataFrames is now complete, allowing for the progression to filtering and cleaning steps.
*   For future Kaggle API interactions, it is crucial to verify dataset IDs and their public accessibility to avoid `403 Client Error` issues, potentially requiring manual checks on the Kaggle website if programmatic access fails.
"""